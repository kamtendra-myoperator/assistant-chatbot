{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concepts\n",
    "\n",
    "### LLM (Large Language Models)\n",
    "Large Language Models refer to advanced machine learning models that are trained on vast amounts of textual data to understand and generate human-like text. \n",
    "These models are pre-trained on a diverse range of internet text and then fine-tuned for specific tasks.\n",
    "\n",
    "### Prompt Engineering\n",
    "Prompt engineering involves crafting effective prompts or input queries to elicit desired responses from language models. \n",
    "This technique is particularly useful when interacting with large language models for specific tasks or generating targeted outputs. \n",
    "Prompt engineering aims to guide the model in producing the desired type of information or response.\n",
    "\n",
    "### RAG (Retrieval-Augmented Generation)\n",
    "Framework designed for natural language processing tasks. It combines retrieval-based models with generative models to enhance the capabilities of language understanding and generation. The RAG pipeline involves two main components: a retriever and a generator.\n",
    "\n",
    "- Retriever\n",
    "  The retriever is responsible for selecting relevant information from a large dataset or knowledge base.\n",
    "  It does this by retrieving a subset of documents that are likely to contain relevant information for a given input or query.\n",
    "  One common approach for retrieval is to use dense vector representations of documents and queries.\n",
    "  Similarity metrics, such as cosine similarity, can then be employed to identify the most relevant documents.\n",
    "\n",
    "- Generator\n",
    "  The generator is a language model capable of producing coherent and contextually relevant text.\n",
    "  It takes the information retrieved by the retriever and generates a response, completing a natural language understanding or generation task.\n",
    "\n",
    "  <img src=\"./rag.png\" alt=\"Vector\" width=\"600\" />\n",
    "\n",
    "### Why Should One Use RAG?\n",
    "There are three ways an LLM can learn new data.\n",
    "\n",
    "- **Training:** A large mesh of neural networks is trained over trillions of tokens with billions of parameters to create Large Language Models. The parameters of a deep learning model are the coefficients or weights that hold all the information regarding the particular model. To train a model like GPT-4 costs hundreds of millions of dollars. This way is beyond anyone’s capacity. We cannot re-train such a humongous model on new data. This is not feasible.\n",
    "- **Fine-tuning:** Another option is to fine-tune a model on existing data. Fine-tuning involves using a pre-trained model as a starting point during training. We use the knowledge of the pre-trained model to train a new model on different data sets. Albeit it is very potent, it is expensive in terms of time and money. Unless there is a specific requirement, fine-tuning does not make sense.\n",
    "- **Prompting:** Prompting is the method where we fit new information within the context window of an LLM and make it answer the queries from the information given in the prompt. It may not be as effective as knowledge learned during training or fine-tuning, but it is sufficient for many real-life use cases, such as document Q&A.\n",
    "<br><br>\n",
    "Prompting for answers from text documents is effective, but these documents are often much larger than the context windows of Large Language Models (LLMs), posing a challenge. Retrieval Augmented Generation (RAG) pipelines address this by processing, storing, and retrieving relevant document sections, allowing LLMs to answer queries efficiently. So, let’s discuss the crucial components of an RAG pipeline.\n",
    "\n",
    "### Embeddings\n",
    "Embedding refers to the process of representing objects or entities (such as words, images, or documents) in a lower-dimensional space.\n",
    "It's basically the data that converted in form of numbers (vector).\n",
    "\n",
    "### Vectorsiation\n",
    "Vectorization is the process of converting data into the vector form.\n",
    "<br>\n",
    "or\n",
    "<br>\n",
    "Process of converting data into embeddings and putting it into vector database.\n",
    "\n",
    "<img src=\"./vector.jpg\" alt=\"Vector\" width=\"400\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo Pipeline\n",
    "<img src=\"./pipeline.png\" alt=\"pipeline\" width=\"600\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain==0.0.284 google-generativeai sentence-transformers langchain_community python-dotenv==1.0.0 streamlit==1.22.0 tiktoken==0.4.0 faiss-cpu==1.7.4 protobuf~=3.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing LLM (Google Palm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GooglePalm\n",
    "\n",
    "api_key = \"AIzaSyDRrLdX40JvYyb8q-LHKu4QdvJgQUsp-_Y\"\n",
    "\n",
    "llm = GooglePalm(google_api_key = api_key, temprature = 0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poem = llm(\"Write four line on India.\")\n",
    "poem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Company's data as CSV\n",
    "\n",
    "Note: It can be loaded and updated in any format according to need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"sample.csv\", source_column=\"prompt\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Embedding using Langchain's InstructEmbeddings\n",
    "\n",
    "Doc: [Instruct Embeddings on Hugging Face](https://python.langchain.com/docs/integrations/text_embedding/instruct_embeddings)\n",
    "\n",
    "Note: Most voted embedding on LangChain leaderboard should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "\n",
    "\n",
    "instructor_embeddings = HuggingFaceInstructEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Embedding Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = instructor_embeddings.embed_query(\"What is your refund policy?\")\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing Embedding in Vector Database (FAISS) \n",
    "\n",
    "Vectorisation\n",
    "\n",
    "Note: Chroma Vector Database should be efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vector_db = FAISS.from_documents(documents=data, embedding=instructor_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval\n",
    "\n",
    "Note: CSV Retriever will be efficient (if csv file)\n",
    "\n",
    "This is the most imp step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Creating Retriever Object \n",
    "- It takes input question\n",
    "- Its compare its embedding with embedding of data in database\n",
    "- Return most suitable response/s using cosine similarity\n",
    "'''\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "#showing relevent docs\n",
    "rdocs= retriever.get_relevant_documents(\"how about job placement support?\", top_k=3)\n",
    "rdocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excracting final response from relevent docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
    "    In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
    "    If the answer is not found in the context, then give most suitable answer from your general knowledge but if it's yes or no question, then answer \"I don't know\".\n",
    "\n",
    "    CONTEXT: {context}\n",
    "\n",
    "    QUESTION: {question}\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type = \"stuff\",\n",
    "    retriever=retriever,\n",
    "    input_key=\"query\",\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "\n",
    "# chain(\"can i take this bootcamp without any prior knowledge of coding?\")\n",
    "# chain(\"is javascript important for frontend development?\")\n",
    "chain(\"do you have javascript course?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
